# coding:utf-8
time_out: 200                  # 设置抓取和存储一个用户资料超时时间
min_crawl_interal: 8           # 页面请求最小间隔
max_crawl_interal: 11          # 页面请求最大间隔
excp_interal: 5*60             # 请求异常的时候睡眠的时间
# TODO 除了可以指定抓取页数以外，也可以不指定抓取页数，就默认抓取全部
max_search_page: 30            # 搜索最多抓多少页
max_home_page: 35              # 主页微博最多抓取页数
max_comment_page: 2000         # 每条微博最大抓取的评论页数（注意不是评论数）
max_repost_page: 2000          # 转发信息最多抓取多少页
max_retries: 5                 # 抓取失败重试次数
yundama_username: ''           # 云打码用户名,如果登录账号全是在常用地登录，则不需要填写
yundama_passwd: ''             # 云打码密码
# 数据采集，分为normal和quick两种，默认采用normal
# 前者用于对数据量追求不大而是需要定时采集某些数据的情况
# 后者应用于需要做大规模数据采集的情况，这种情况下一个账号运行两到三个小时应该会被封
mode: normal
# 如果模式是quick，请设置一个cookie可以同时被多少个IP共享，微博系统目前支持一个cookie同时能够在5台主机上共享
share_host_count: 5



db:
    host: 127.0.0.1
    port: 3306
    user: root
    password: 123456
    db_name: weibo
    db_type: mysql

# 保存登录的cookies
redis:
    host: 127.0.0.1
    port: 6379
    password: ''
    cookies: 1 # cookie存取
    urls: 2 # 抓取过的url
    broker: 5 # celery的broker
    backend: 6 # celery的backend
    id_name: 8 # user_id和对应的name，这个是为转发微博分析而存在的




